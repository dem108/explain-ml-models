{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Interpretability Dashboard with Employee Attrition Data\n",
    "\n",
    "This notebook illustrates creating explanations for a binary classification model, employee attrition classification, that uses one to one and one to many feature transformations from raw data to engineered features. It will showcase raw feature transformations with three tabular data explainers: TabularExplainer (SHAP), MimicExplainer (global surrogate), and PFIExplainer.\n",
    "\n",
    "Problem: Employee attrition classification with scikit-learn (run model explainer locally)\n",
    "\n",
    "1. Transform raw features to engineered features.\n",
    "2. Train a classification model using Scikit-learn.\n",
    "3. Run 'explain_model' globally and locally with full dataset.\n",
    "4. Visualize the global and local explanations with the interpretability visualization dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting interpret-community\n",
      "  Using cached interpret_community-0.17.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied, skipping upgrade: scipy in c:\\users\\sehan\\appdata\\roaming\\python\\python37\\site-packages (from interpret-community) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from interpret-community) (1.16.5)\n",
      "Collecting shap<=0.34.0,>=0.20.0\n",
      "  Using cached shap-0.34.0-cp37-cp37m-win_amd64.whl (298 kB)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from interpret-community) (0.21.3)\n",
      "Requirement already satisfied, skipping upgrade: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from interpret-community) (19.2)\n",
      "Collecting interpret-core[required]<=0.2.4,>=0.1.20\n",
      "  Using cached interpret_core-0.2.4-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied, skipping upgrade: pandas in c:\\users\\sehan\\appdata\\roaming\\python\\python37\\site-packages (from interpret-community) (0.25.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>4.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap<=0.34.0,>=0.20.0->interpret-community) (4.36.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\sehan\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn->interpret-community) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->interpret-community) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->interpret-community) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->interpret-community) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->interpret-community) (2.8.0)\n",
      "Installing collected packages: shap, interpret-core, interpret-community\n",
      "Successfully installed interpret-community-0.17.1 interpret-core-0.2.4 shap-0.34.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\ProgramData\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade interpret-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing packages, you must close and reopen the notebook as well as restarting the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "### Run model explainer at training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the employee attrition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdirname = 'dataset.6.21.19'\n",
    "zipfilename = outdirname + '.zip'\n",
    "urlretrieve('https://publictestdatasets.blob.core.windows.net/data/' + zipfilename, zipfilename)\n",
    "with zipfile.ZipFile(zipfilename, 'r') as unzip:\n",
    "    unzip.extractall('.')\n",
    "attritionData = pd.read_csv('./WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "\n",
    "# Dropping Employee count as all values are 1 and hence attrition is independent of this feature\n",
    "attritionData = attritionData.drop(['EmployeeCount'], axis=1)\n",
    "# Dropping Employee Number since it is merely an identifier\n",
    "attritionData = attritionData.drop(['EmployeeNumber'], axis=1)\n",
    "attritionData = attritionData.drop(['Over18'], axis=1)\n",
    "\n",
    "# Since all values are 80\n",
    "attritionData = attritionData.drop(['StandardHours'], axis=1)\n",
    "\n",
    "# Converting target variables from string to numerical values\n",
    "target_map = {'Yes': 0, 'No': 1}\n",
    "attritionData[\"Attrition_numerical\"] = attritionData[\"Attrition\"].apply(lambda x: target_map[x])\n",
    "target = attritionData[\"Attrition_numerical\"]\n",
    "\n",
    "attritionXData = attritionData.drop(['Attrition_numerical', 'Attrition'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(attritionXData, \n",
    "                                                    target, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy columns for each categorical feature\n",
    "categorical = []\n",
    "for col, value in attritionXData.iteritems():\n",
    "    if value.dtype == 'object':\n",
    "        categorical.append(col)\n",
    "        \n",
    "# Store the numerical columns in a list numerical\n",
    "numerical = attritionXData.columns.difference(categorical)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform raw features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explain raw features by either using a `sklearn.compose.ColumnTransformer` or a list of fitted transformer tuples. The cell below uses `sklearn.compose.ColumnTransformer`. In case you want to run the example with the list of fitted transformer tuples, comment the cell below and uncomment the cell that follows after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        ('cat', categorical_transformer, categorical)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', transformations),\n",
    "                      ('classifier', LGBMClassifier())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a LightGBM classification model, which you want to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain your model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Could not import LIME, required for LIMEExplainer\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    }
   ],
   "source": [
    "# clf.steps[-1][1] returns the trained classification model\n",
    "\n",
    "# 1. Using SHAP TabularExplainer\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "explainer = TabularExplainer(clf.steps[-1][1], \n",
    "                             initialization_examples=x_train, \n",
    "                             features=attritionXData.columns, \n",
    "                             classes=['Leaving', 'Staying'], \n",
    "                             transformations=transformations)\n",
    "\n",
    "\n",
    "# 2. Using MimicExplainer\n",
    "# augment_data is optional and if true, oversamples the initialization examples to improve surrogate model accuracy to fit original model.  Useful for high-dimensional data where the number of rows is less than the number of columns. \n",
    "# max_num_of_augmentations is optional and defines max number of times we can increase the input data size.\n",
    "# LGBMExplainableModel can be replaced with LinearExplainableModel, SGDExplainableModel, or DecisionTreeExplainableModel\n",
    "# from interpret.ext.blackbox import MimicExplainer\n",
    "# from interpret.ext.glassbox import LGBMExplainableModel\n",
    "# explainer = MimicExplainer(clf.steps[-1][1], \n",
    "#                            x_train, \n",
    "#                            LGBMExplainableModel, \n",
    "#                            augment_data=True, \n",
    "#                            max_num_of_augmentations=10, \n",
    "#                            features=attritionXData.columns, \n",
    "#                            classes=[\"Leaving\", \"Staying\"], \n",
    "#                            transformations=transformations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Using PFIExplainer\n",
    "# Use the parameter \"metric\" to pass a metric name or function to evaluate the permutation. \n",
    "# Note that if a metric function is provided a higher value must be better.\n",
    "# Otherwise, take the negative of the function or set the parameter \"is_error_metric\" to True.\n",
    "# Default metrics: \n",
    "# F1 Score for binary classification, F1 Score with micro average for multiclass classification and\n",
    "# Mean absolute error for regression\n",
    "# from interpret.ext.blackbox import PFIExplainer\n",
    "# explainer = PFIExplainer(clf.steps[-1][1], \n",
    "#                          features=x_train.columns, \n",
    "#                          transformations=transformations,\n",
    "#                          classes=[\"Leaving\", \"Staying\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate global explanations\n",
    "Explain overall model predictions (global explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    }
   ],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# x_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "global_explanation = explainer.explain_global(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "global importance rank: {'OverTime': 0.8671180034119259, 'StockOptionLevel': 0.5935162276276619, 'MonthlyIncome': 0.5077163466162175, 'EnvironmentSatisfaction': 0.38370025137840846, 'NumCompaniesWorked': 0.3621168979475014, 'DailyRate': 0.3294469364706887, 'DistanceFromHome': 0.32849406911313805, 'YearsWithCurrManager': 0.32023648645055924, 'JobRole': 0.2909562918384063, 'BusinessTravel': 0.2817429228713378, 'JobSatisfaction': 0.27726380403494433, 'JobInvolvement': 0.27331413643868785, 'Age': 0.2646025452416837, 'EducationField': 0.219575214376005, 'TotalWorkingYears': 0.18424426064629487, 'YearsSinceLastPromotion': 0.1717552999308561, 'PercentSalaryHike': 0.16082737771731798, 'RelationshipSatisfaction': 0.1588264184437708, 'MonthlyRate': 0.15783505069727408, 'WorkLifeBalance': 0.15720113204387484, 'HourlyRate': 0.14775935354581018, 'MaritalStatus': 0.14756350726759088, 'YearsAtCompany': 0.14555452717451922, 'JobLevel': 0.14419841695938582, 'TrainingTimesLastYear': 0.13421152646827703, 'YearsInCurrentRole': 0.1330933759835596, 'Department': 0.12368018826746925, 'Gender': 0.06843374611733986, 'Education': 0.03513064245741514, 'PerformanceRating': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Print out a dictionary that holds the sorted feature importance names and values\n",
    "print('global importance rank: {}'.format(global_explanation.get_feature_importance_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate local explanations\n",
    "Explain local data points (individual instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    }
   ],
   "source": [
    "# You can pass a specific data point or a group of data points to the explain_local function\n",
    "# E.g., Explain the first data point in the test set\n",
    "instance_num = 1\n",
    "local_explanation = explainer.explain_local(x_test[:instance_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = clf.predict(x_test)[instance_num]\n",
    "\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()[prediction_value]\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()[prediction_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "local importance values: [[0.9872782509670005, 0.6098483362087465, 0.49360776366437137, 0.3958696927838185, 0.2511517496176293, 0.22759400856616324, 0.14984831748112823, 0.11269249192142855, 0.07753491402246747, 0.05533656450429863, 0.04854240809166762, 0.04119399537275928, 0.03863692290194614, 0.0116183804145255, 0.0, -0.000829737976490213, -0.04020280699078922, -0.04137092837165063, -0.0870175410207079, -0.09508504778043447, -0.10265623437460784, -0.11253826053335347, -0.1570362620333481, -0.19090440616256293, -0.27674855535331966, -0.32846991637164863, -0.6214704096223682, -0.6348852178684176, -1.1339969326551331, -1.1737571991243123]]\n",
      "local importance names: [['OverTime', 'MaritalStatus', 'StockOptionLevel', 'EnvironmentSatisfaction', 'DistanceFromHome', 'BusinessTravel', 'TotalWorkingYears', 'WorkLifeBalance', 'JobInvolvement', 'YearsSinceLastPromotion', 'Education', 'TrainingTimesLastYear', 'RelationshipSatisfaction', 'PercentSalaryHike', 'PerformanceRating', 'YearsInCurrentRole', 'Gender', 'HourlyRate', 'DailyRate', 'NumCompaniesWorked', 'JobLevel', 'YearsAtCompany', 'MonthlyRate', 'JobSatisfaction', 'Department', 'Age', 'JobRole', 'EducationField', 'MonthlyIncome', 'YearsWithCurrManager']]\n"
     ]
    }
   ],
   "source": [
    "print('local importance values: {}'.format(sorted_local_importance_values))\n",
    "print('local importance names: {}'.format(sorted_local_importance_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Load the interpretability visualization dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'raiwidgets'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8d514ee51851>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mraiwidgets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExplanationDashboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'raiwidgets'"
     ]
    }
   ],
   "source": [
    "from raiwidgets import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, model, dataset=x_test, true_y=y_test.values)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "mesameki"
   }
  ],
  "kernelspec": {
   "display_name": "Python (explain_ml)",
   "language": "python",
   "name": "explain_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}